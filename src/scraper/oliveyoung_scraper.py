"""Olive Young ì œí’ˆ ì •ë³´ ìŠ¤í¬ë˜í¼ (Playwright ê¸°ë°˜)"""
import asyncio
import json
from pathlib import Path
from typing import Optional
from loguru import logger
from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import Stealth


class ProductInfo:
    """ì œí’ˆ ì •ë³´ ë°ì´í„° í´ë˜ìŠ¤"""
    def __init__(self):
        self.name: Optional[str] = None
        self.price: Optional[str] = None
        self.rating: Optional[str] = None
        self.review_count: Optional[str] = None
        self.detail_images: list[str] = []
        self.review_rating_distribution: dict[int, str] = {}
        self.reviews: list[str] = []
        self.review_ratings: list[str] = []


class OliveYoungScraper:
    """Olive Young ì œí’ˆ í˜ì´ì§€ ìŠ¤í¬ë˜í¼ (Playwright ê¸°ë°˜)"""

    SELECTORS = {
        "name": "#Contents > div.prd_detail_box.renew > div.right_area > div > p.prd_name",
        "regular_price": "#Contents > div.prd_detail_box.renew > div.right_area > div > div.price > span.price-1 > strike",
        "discount_price": "#Contents > div.prd_detail_box.renew > div.right_area > div > div.price > span.price-2 > strong",
        "rating": "#repReview > b",
        "review_count": "#repReview > em",
        "detail_toggle": "#btn_toggle_detail_image",
        "review_button": "#reviewInfo > a",
        "sort_by_helpfulness_button": "#gdasSort > li:nth-child(2) > a",
    }

    def __init__(self, headless: bool = False):
        """
        Args:
            headless: ë¸Œë¼ìš°ì €ë¥¼ headless ëª¨ë“œë¡œ ì‹¤í–‰í• ì§€ ì—¬ë¶€
        """
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None

    async def __aenter__(self):
        """Async context manager entry"""
        self.playwright = await async_playwright().start()

        # ì‹¤ì œ Chrome ë¸Œë¼ìš°ì € ì‚¬ìš© (channel='chrome')
        self.browser = await self.playwright.chromium.launch(
            headless=self.headless,
            channel='chrome',
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
                '--disable-web-security',
                '--disable-features=IsolateOrigins,site-per-process',
                '--disable-site-isolation-trials',
                '--disable-features=BlockInsecurePrivateNetworkRequests'
            ]
        )

        # Context ìƒì„± (User Agent ë° Viewport ì„¤ì •)
        context = await self.browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            viewport={"width": 1920, "height": 1080},
            locale="ko-KR",
            timezone_id="Asia/Seoul",
            permissions=["geolocation"],
            geolocation={"longitude": 126.9780, "latitude": 37.5665},
            extra_http_headers={
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
                "Accept-Encoding": "gzip, deflate, br",
                "Sec-Ch-Ua": '"Chromium";v="131", "Not_A Brand";v="24"',
                "Sec-Ch-Ua-Mobile": "?0",
                "Sec-Ch-Ua-Platform": '"macOS"',
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Upgrade-Insecure-Requests": "1"
            }
        )

        self.page = await context.new_page()

        # ì¿ í‚¤ ë¡œë“œ ë° ì£¼ì…
        await self._load_cookies()

        # Stealth ëª¨ë“œ ì ìš©
        stealth = Stealth()
        await stealth.apply_stealth_async(self.page)

        # ì¶”ê°€ JavaScript ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸
        await self.page.add_init_script("""
            // WebDriver ì†ì„± ì œê±°
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });

            // Chrome ê°ì²´ ì¶”ê°€
            window.chrome = {
                runtime: {}
            };

            // Permissions API ìš°íšŒ
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                    Promise.resolve({ state: Notification.permission }) :
                    originalQuery(parameters)
            );

            // Plugin ë°°ì—´ ì„¤ì •
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5]
            });

            // Languages ì„¤ì •
            Object.defineProperty(navigator, 'languages', {
                get: () => ['ko-KR', 'ko', 'en-US', 'en']
            });
        """)

        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.page:
            await self.page.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()

    async def scrape(self, url: str, max_reviews: int = 30) -> ProductInfo:
        """
        ì œí’ˆ í˜ì´ì§€ì—ì„œ ì •ë³´ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.

        Args:
            url: Olive Young ì œí’ˆ í˜ì´ì§€ URL
            max_reviews: ìµœëŒ€ ìŠ¤í¬ë˜í•‘í•  ë¦¬ë·° ê°œìˆ˜

        Returns:
            ProductInfo: ìŠ¤í¬ë˜í•‘ëœ ì œí’ˆ ì •ë³´
        """
        if not self.page:
            raise RuntimeError("Scraper must be used as async context manager")

        logger.info(f"Scraping URL: {url}")

        # í˜ì´ì§€ ë¡œë”© (loadê¹Œì§€ë§Œ ëŒ€ê¸°)
        await self.page.goto(url, wait_until="load", timeout=60000)
        logger.info("í˜ì´ì§€ ì´ˆê¸° ë¡œë”© ì™„ë£Œ, Cloudflare ì²´í¬ ëŒ€ê¸° ì¤‘...")

        # Cloudflare ì²´í¬ ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
        try:
            # Cloudflare ë¡œë”© í™”ë©´ì´ ì‚¬ë¼ì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸°
            await self.page.wait_for_function(
                """() => {
                    const bodyText = document.body.innerText;
                    // Cloudflare ì²´í¬ í™”ë©´ì´ ì•„ë‹Œì§€ í™•ì¸
                    return !bodyText.includes('ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”') &&
                           !bodyText.includes('í™•ì¸ ì¤‘') &&
                           !bodyText.includes('Checking your browser');
                }""",
                timeout=30000
            )
            logger.info("âœ… Cloudflare ì²´í¬ í†µê³¼!")
        except Exception as e:
            logger.warning(f"Cloudflare ì²´í¬ ëŒ€ê¸° ì¤‘ íƒ€ì„ì•„ì›ƒ (ê³„ì† ì§„í–‰): {e}")

        # ì¶”ê°€ ì•ˆì •í™” ëŒ€ê¸°
        await self.page.wait_for_timeout(3000)

        # ë§ˆìš°ìŠ¤ ì›€ì§ì„ ì‹œë®¬ë ˆì´ì…˜ (ë´‡ íƒì§€ ìš°íšŒ)
        await self.page.mouse.move(100, 100)
        await self.page.wait_for_timeout(500)
        await self.page.mouse.move(300, 300)
        await self.page.wait_for_timeout(500)

        # ìŠ¤í¬ë¡¤ ì‹œë®¬ë ˆì´ì…˜
        await self.page.evaluate("window.scrollTo(0, 200)")
        await self.page.wait_for_timeout(1000)
        await self.page.evaluate("window.scrollTo(0, 0)")
        await self.page.wait_for_timeout(1000)

        product = ProductInfo()

        # ì œí’ˆëª…
        try:
            await self.page.wait_for_timeout(1000)  # ì¶”ê°€ ëŒ€ê¸°
            product.name = await self._get_text(self.SELECTORS["name"])
            logger.info(f"ì œí’ˆëª…: {product.name}")
        except Exception as e:
            logger.warning(f"ì œí’ˆëª… ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

        # ê°€ê²©
        try:
            product.price = await self._get_price()
            logger.info(f"ê°€ê²©: {product.price}")
        except Exception as e:
            logger.warning(f"ê°€ê²© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

        # ë¦¬ë·° í‰ì 
        try:
            product.rating = await self._get_text(self.SELECTORS["rating"])
            logger.info(f"í‰ì : {product.rating}")
        except Exception as e:
            logger.warning(f"í‰ì  ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

        # ë¦¬ë·° ê°œìˆ˜
        try:
            product.review_count = await self._get_text(self.SELECTORS["review_count"])
            logger.info(f"ë¦¬ë·° ê°œìˆ˜: {product.review_count}")
        except Exception as e:
            logger.warning(f"ë¦¬ë·° ê°œìˆ˜ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

        # ì œí’ˆ ìƒì„¸ì •ë³´ ì´ë¯¸ì§€
        try:
            product.detail_images = await self._get_detail_images()
            logger.info(f"ìƒì„¸ ì´ë¯¸ì§€ ê°œìˆ˜: {len(product.detail_images)}")
        except Exception as e:
            logger.warning(f"ìƒì„¸ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")

        # ë¦¬ë·° íƒ­ í´ë¦­, í‰ì  ë¶„í¬ ê°€ì ¸ì˜¤ê¸°, ì •ë ¬ ë° ì¶”ì¶œ
        try:
            await self._click_review_tab()
            product.review_rating_distribution = await self._get_review_rating_distribution()
            await self._sort_reviews_by_helpfulness()

            # í˜ì´ì§€ë„¤ì´ì…˜í•˜ë©° ëª¨ë“  ë¦¬ë·° ì¶”ì¶œ
            product.reviews, product.review_ratings = await self._paginate_and_extract_reviews(max_reviews=max_reviews)
            logger.info(f"ì´ {len(product.reviews)}ê°œì˜ ë¦¬ë·° ì¶”ì¶œ ì™„ë£Œ")

        except Exception as e:
            logger.warning(f"ë¦¬ë·° ì •ë³´ ê°€ì ¸ì˜¤ê¸°, ì •ë ¬ ë° ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return product

    async def _paginate_and_extract_reviews(self, max_reviews: int) -> tuple[list[str], list[str]]:
        """ëª¨ë“  ë¦¬ë·° í˜ì´ì§€ë¥¼ ëŒë©° ìµœëŒ€ max_reviewsê°œê¹Œì§€ ë¦¬ë·°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        all_reviews = []
        all_ratings = []
        while len(all_reviews) < max_reviews:
            # í˜„ì¬ í˜ì´ì§€ ë¦¬ë·° ì¶”ì¶œ
            reviews_on_page, ratings_on_page = await self._extract_reviews_from_page()
            if not reviews_on_page:
                logger.info("í˜„ì¬ í˜ì´ì§€ì— ë¦¬ë·°ê°€ ì—†ì–´ í˜ì´ì§€ë„¤ì´ì…˜ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            
            for review, rating in zip(reviews_on_page, ratings_on_page):
                if len(all_reviews) < max_reviews:
                    all_reviews.append(review)
                    all_ratings.append(rating)
                else:
                    break
            
            if len(all_reviews) >= max_reviews:
                logger.info(f"ìµœëŒ€ ë¦¬ë·° ê°œìˆ˜({max_reviews})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                break

            # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
            try:
                paging_container = "#gdasContentsArea > div > div.pageing"
                # í˜„ì¬ í™œì„±í™”ëœ í˜ì´ì§€(<strong>)ë¥¼ ì°¾ìŒ
                current_page_element = await self.page.query_selector(f"{paging_container} > strong")
                if not current_page_element:
                    logger.info("í™œì„±í™”ëœ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
                
                current_page_num = int((await current_page_element.text_content()).strip())

                # ë‹¤ìŒ ë²„íŠ¼ ì°¾ê¸°
                next_button = None
                # í˜„ì¬ í˜ì´ì§€ê°€ 10, 20, ... ì´ë©´ 'ë‹¤ìŒ' ë²„íŠ¼ í´ë¦­
                if current_page_num % 10 == 0:
                    next_button = await self.page.query_selector(f"{paging_container} > a.next")
                # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë°”ë¡œ ë‹¤ìŒ ë²ˆí˜¸(<strong> ë°”ë¡œ ë‹¤ìŒ <a>) í´ë¦­
                else:
                    next_button = await self.page.query_selector(f"{paging_container} > strong + a")

                if not next_button:
                    logger.info("ë§ˆì§€ë§‰ í˜ì´ì§€ì…ë‹ˆë‹¤. ë¦¬ë·° ì¶”ì¶œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

                button_text = (await next_button.text_content()).strip()
                logger.info(f"ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤: '{button_text}'")
                await next_button.click()
                # TODO: í˜ì´ì§€ ë¡œë”©ì„ ë” ì•ˆì •ì ìœ¼ë¡œ ê¸°ë‹¤ë¦¬ëŠ” ë°©ë²•ìœ¼ë¡œ ê°œì„  í•„ìš”
                await self.page.wait_for_timeout(2000)

            except Exception as e:
                logger.warning(f"í˜ì´ì§€ ì´ë™ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. í˜ì´ì§€ë„¤ì´ì…˜ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
        
        return all_reviews, all_ratings

    async def _extract_reviews_from_page(self) -> tuple[list[str], list[str]]:
        """í˜„ì¬ í˜ì´ì§€ì˜ ë¦¬ë·° í…ìŠ¤íŠ¸ì™€ ë³„ì ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        reviews_on_page = []
        ratings_on_page = []
        logger.info("í˜„ì¬ í˜ì´ì§€ì˜ ë¦¬ë·° ì¶”ì¶œ ì‹œì‘")
        try:
            review_list_selector = "#gdasList"
            await self.page.wait_for_selector(review_list_selector, timeout=5000)

            review_elements = await self.page.query_selector_all(f"{review_list_selector} > li")
            logger.info(f"í˜„ì¬ í˜ì´ì§€ì—ì„œ {len(review_elements)}ê°œì˜ ë¦¬ë·° í•­ëª©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

            for i in range(1, len(review_elements) + 1):
                review_text_selector = f"#gdasList > li:nth-child({i}) > div.review_cont > div.txt_inner"
                review_rating_selector = f"#gdasList > li:nth-child({i}) > div.review_cont > div.score_area > span.review_point > span"
                
                try:
                    # ë¦¬ë·° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text = await self._get_text(review_text_selector, timeout=1000)
                    # ë¦¬ë·° ë³„ì  ì¶”ì¶œ
                    rating_text = await self._get_text(review_rating_selector, timeout=1000)
                    rating = self._parse_rating_from_text(rating_text)
                    
                    reviews_on_page.append(text)
                    ratings_on_page.append(rating)
                    logger.debug(f"{i}ë²ˆì§¸ ë¦¬ë·° ì¶”ì¶œ ì„±ê³µ. ë³„ì : {rating}")
                except Exception:
                    # txt_innerê°€ ì—†ëŠ” ê²½ìš° (e.g. í¬í† ë¦¬ë·°)ëŠ” í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë¯€ë¡œ ê±´ë„ˆëœ€
                    logger.warning(f"{i}ë²ˆì§¸ ë¦¬ë·°ì—ì„œ í…ìŠ¤íŠ¸(.txt_inner) ë˜ëŠ” ë³„ì ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í¬í† ë¦¬ë·°ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            logger.error(f"ë¦¬ë·° ëª©ë¡ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        logger.info(f"í˜„ì¬ í˜ì´ì§€ì—ì„œ {len(reviews_on_page)}ê°œì˜ ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
        return reviews_on_page, ratings_on_page

    async def _sort_reviews_by_helpfulness(self):
        """ë¦¬ë·°ë¥¼ 'ë„ì›€ìˆœ'ìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤."""
        logger.info("ë¦¬ë·°ë¥¼ 'ë„ì›€ìˆœ'ìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.")
        try:
            sort_button = await self.page.wait_for_selector(
                self.SELECTORS["sort_by_helpfulness_button"],
                timeout=5000
            )
            await sort_button.click()
            logger.info("'ë„ì›€ìˆœ' ì •ë ¬ ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
            # ì •ë ¬ í›„ ë¦¬ë·° ëª©ë¡ì´ ìƒˆë¡œê³ ì¹¨ë  ë•Œê¹Œì§€ ëŒ€ê¸°
            # TODO: ë” ì•ˆì •ì ì¸ ëŒ€ê¸° ë°©ë²• (e.g., network idle or a specific element change)
            await self.page.wait_for_timeout(2000)
        except Exception as e:
            logger.warning(f"'ë„ì›€ìˆœ'ìœ¼ë¡œ ì •ë ¬í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            await self.page.screenshot(path="debug_screenshot_sort_fail.png")
            logger.info("ë””ë²„ê¹… ìŠ¤í¬ë¦°ìƒ· ì €ì¥: debug_screenshot_sort_fail.png")

    async def _get_review_rating_distribution(self) -> dict[int, str]:
        """ê° ë³„ì ë³„ ë¦¬ë·° ë¶„í¬(%)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        distribution = {}
        logger.info("ë¦¬ë·° í‰ì ë³„ ë¶„í¬ ê°€ì ¸ì˜¤ê¸° ì‹œì‘")
        try:
            graph_area_selector = "#gdasContentsArea > div > div.product_rating_area.review-write-delete > div > div.graph_area"
            await self.page.wait_for_selector(graph_area_selector, timeout=5000)

            for i in range(1, 6):  # li:nth-child(1) to li:nth-child(5)
                rating = 6 - i  # 5 stars to 1 star
                selector = f"{graph_area_selector} > ul > li:nth-child({i}) > span.per"
                percentage_text = await self._get_text(selector, timeout=1000)
                if percentage_text:
                    distribution[rating] = percentage_text
                    logger.info(f"{rating}ì  ë¦¬ë·° ë¹„ìœ¨: {percentage_text}")
                else:
                    logger.warning(f"{rating}ì  ë¦¬ë·° ë¹„ìœ¨ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            logger.warning(f"ë¦¬ë·° í‰ì ë³„ ë¶„í¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return distribution

    async def _click_review_tab(self):
        """ë¦¬ë·° íƒ­ì„ í´ë¦­í•˜ì—¬ ë¦¬ë·° ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            review_button = await self.page.wait_for_selector(
                self.SELECTORS["review_button"],
                timeout=10000
            )
            await review_button.click()
            logger.info("ë¦¬ë·° íƒ­ í´ë¦­ ì„±ê³µ")
            # ë¦¬ë·°ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤.
            # TODO: ë¦¬ë·° ì»¨í…Œì´ë„ˆê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ” ë” ì•ˆì •ì ì¸ ë°©ë²•ìœ¼ë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.
            await self.page.wait_for_timeout(2000)
            logger.info("ë¦¬ë·° ì •ë³´ ë¡œë”© ëŒ€ê¸° ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ë¦¬ë·° íƒ­ í´ë¦­ ì‹¤íŒ¨: {e}")
            await self.page.screenshot(path="debug_screenshot_review_click_fail.png")
            logger.info("ë””ë²„ê¹… ìŠ¤í¬ë¦°ìƒ· ì €ì¥: debug_screenshot_review_click_fail.png")

    async def _get_price(self) -> str:
        """ê°€ê²©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. í• ì¸ê°€ê°€ ìˆìœ¼ë©´ í• ì¸ê°€ë¥¼, ì—†ìœ¼ë©´ ì •ê°€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            # ë¨¼ì € í• ì¸ê°€ ì‹œë„
            try:
                discount_price = await self._get_text(self.SELECTORS["discount_price"], timeout=2000)
                if discount_price:
                    logger.info(f"í• ì¸ê°€ ë°œê²¬: {discount_price}")
                    return discount_price
            except Exception:
                logger.debug("í• ì¸ê°€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # í• ì¸ê°€ê°€ ì—†ìœ¼ë©´ ì •ê°€ ì‹œë„
            try:
                regular_price = await self._get_text(self.SELECTORS["regular_price"], timeout=2000)
                if regular_price:
                    logger.info(f"ì •ê°€ ë°œê²¬: {regular_price}")
                    return regular_price
            except Exception:
                logger.debug("ì •ê°€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            logger.warning("ì •ê°€ì™€ í• ì¸ê°€ ëª¨ë‘ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return ""
            
        except Exception as e:
            logger.warning(f"ê°€ê²© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return ""

    def _parse_rating_from_text(self, rating_text: str) -> str:
        """'5ì ë§Œì ì— xì ' í˜•ì‹ì˜ í…ìŠ¤íŠ¸ì—ì„œ ë³„ì ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            if "ì ë§Œì ì—" in rating_text and "ì " in rating_text:
                # '5ì ë§Œì ì— 4ì ' -> '4' ì¶”ì¶œ
                parts = rating_text.split("ì ë§Œì ì—")
                if len(parts) > 1:
                    score_part = parts[1].replace("ì ", "").strip()
                    return score_part
            return ""
        except Exception as e:
            logger.warning(f"ë³„ì  íŒŒì‹± ì‹¤íŒ¨: {rating_text}, ì—ëŸ¬: {e}")
            return ""

    async def _get_text(self, selector: str, timeout: int = 10000) -> str:
        """CSS selectorë¡œ ìš”ì†Œì˜ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        element = await self.page.wait_for_selector(selector, state='attached', timeout=timeout)
        text = await element.text_content()
        return text.strip() if text else ""

    async def _get_detail_images(self) -> list[str]:
        """ì œí’ˆ ìƒì„¸ ì´ë¯¸ì§€ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        # í˜ì´ì§€ë¥¼ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤í•˜ì—¬ ìƒì„¸ì •ë³´ ì˜ì—­ ë¡œë”©
        await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight / 2)")
        await self.page.wait_for_timeout(2000)

        # ìƒì„¸ì •ë³´ í† ê¸€ ë²„íŠ¼ í´ë¦­
        try:
            toggle_btn = await self.page.wait_for_selector(
                self.SELECTORS["detail_toggle"],
                timeout=10000
            )
            await toggle_btn.click()
            logger.info("ìƒì„¸ì •ë³´ í† ê¸€ ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
            await self.page.wait_for_timeout(3000)  # ì´ë¯¸ì§€ ë¡œë”© ëŒ€ê¸°
        except Exception as e:
            logger.warning(f"ìƒì„¸ì •ë³´ í† ê¸€ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")
            await self.page.screenshot(path="debug_screenshot.png")
            logger.info("ë””ë²„ê¹… ìŠ¤í¬ë¦°ìƒ· ì €ì¥: debug_screenshot.png")
            return []

        # ëª¨ë“  ì´ë¯¸ì§€ URL ìˆ˜ì§‘ - ë‹¤ì¤‘ ì…€ë ‰í„° íŒ¨í„´ ì‚¬ìš©
        images = []
        logger.info("ìƒì„¸ ì´ë¯¸ì§€ ìˆ˜ì§‘ ì‹œì‘")
        
        # ë‹¤ì–‘í•œ ì œí’ˆ í˜ì´ì§€ êµ¬ì¡°ì— ëŒ€ì‘í•˜ê¸° ìœ„í•œ ì…€ë ‰í„° íŒ¨í„´ë“¤
        selector_patterns = [
            # ìµœì‹  êµ¬ì²´ì  íŒ¨í„´ (ìµœìš°ì„ )
            {"container": "#tempHtml2 > center", "selector": "#tempHtml2 > center > div:nth-child(1) > img:nth-child(1)"},
            {"container": "#tempHtml2 > center", "selector": "#tempHtml2 > center > div img"},
            {"container": "#tempHtml2 > center", "selector": "#tempHtml2 > center img"},
            # ê¸°ë³¸ íŒ¨í„´
            {"container": "#tempHtml2", "selector": "#tempHtml2 > div"},
            {"container": "#tempHtml2", "selector": "#tempHtml2 img"},
            # ëŒ€ì²´ íŒ¨í„´ë“¤
            {"container": "#tempHtml", "selector": "#tempHtml > div"},
            {"container": "#tempHtml", "selector": "#tempHtml img"},
            {"container": ".detail_info_wrap", "selector": ".detail_info_wrap img"},
            {"container": ".prd_detail_info", "selector": ".prd_detail_info img"},
            {"container": ".goods_detail_wrap", "selector": ".goods_detail_wrap img"},
        ]
        
        for pattern in selector_patterns:
            try:
                container_selector = pattern["container"]
                img_selector = pattern["selector"]
                
                # ì»¨í…Œì´ë„ˆê°€ ì§€ì •ëœ ê²½ìš° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                if container_selector:
                    containers = await self.page.query_selector_all(container_selector)
                    if not containers:
                        logger.debug(f"ì»¨í…Œì´ë„ˆ '{container_selector}' ì—†ìŒ, ë‹¤ìŒ íŒ¨í„´ ì‹œë„")
                        continue
                
                # div ì»¨í…Œì´ë„ˆ ê¸°ë°˜ íŒ¨í„´
                if " > div" in img_selector:
                    div_containers = await self.page.query_selector_all(img_selector)
                    logger.debug(f"íŒ¨í„´ '{img_selector}': {len(div_containers)}ê°œ div ì»¨í…Œì´ë„ˆ ë°œê²¬")
                    
                    for i, div_container in enumerate(div_containers, 1):
                        imgs_in_div = await div_container.query_selector_all("img")
                        logger.debug(f"Div ì»¨í…Œì´ë„ˆ #{i}ì—ì„œ {len(imgs_in_div)}ê°œì˜ img íƒœê·¸ ë°œê²¬")

                        for img in imgs_in_div:
                            src = await self._extract_image_src(img)
                            if src and src not in images:
                                images.append(src)
                                logger.debug(f"ì´ë¯¸ì§€ ì¶”ê°€: {src}")
                
                # ì§ì ‘ ì´ë¯¸ì§€ ì…€ë ‰í„° íŒ¨í„´
                else:
                    img_elements = await self.page.query_selector_all(img_selector)
                    logger.debug(f"íŒ¨í„´ '{img_selector}': {len(img_elements)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
                    
                    for img in img_elements:
                        src = await self._extract_image_src(img)
                        if src and src not in images:
                            images.append(src)
                            logger.debug(f"ì´ë¯¸ì§€ ì¶”ê°€: {src}")
                
                # ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ë‹¤ë©´ ì¶”ê°€ íŒ¨í„´ ì‹œë„ ì¤‘ë‹¨
                if images:
                    logger.info(f"íŒ¨í„´ '{img_selector}'ì—ì„œ {len(images)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
                    break
                    
            except Exception as e:
                logger.debug(f"íŒ¨í„´ '{pattern['selector']}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        if images:
            logger.info(f"ì´ {len(images)}ê°œì˜ ìƒì„¸ ì´ë¯¸ì§€ URL ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            logger.warning("ëª¨ë“  ì…€ë ‰í„° íŒ¨í„´ì—ì„œ ìƒì„¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            # ë””ë²„ê¹…ì„ ìœ„í•´ ìŠ¤í¬ë¦°ìƒ·ê³¼ HTML ì €ì¥
            await self.page.screenshot(path="debug_screenshot_detail_img.png")
            html_content = await self.page.content()
            with open("debug_page_source_detail_img.html", "w", encoding="utf-8") as f:
                f.write(html_content)
            logger.info("ìƒì„¸ ì´ë¯¸ì§€ ë””ë²„ê¹… íŒŒì¼ ì €ì¥ ì™„ë£Œ")

        return images

    async def _extract_image_src(self, img_element) -> Optional[str]:
        """ì´ë¯¸ì§€ ìš”ì†Œì—ì„œ src ì†ì„±ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            # ë‹¤ì–‘í•œ src ì†ì„± ì‹œë„
            src = await img_element.get_attribute("src")
            if not src or "http" not in src:
                src = await img_element.get_attribute("data-src")
            if not src or "http" not in src:
                src = await img_element.get_attribute("data-original")
            if not src or "http" not in src:
                src = await img_element.get_attribute("data-lazy")

            return src if src and "http" in src else None
        except Exception as e:
            logger.debug(f"ì´ë¯¸ì§€ src ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def _load_cookies(self):
        """cookies.json íŒŒì¼ì—ì„œ ì¿ í‚¤ë¥¼ ë¡œë“œí•˜ì—¬ í˜ì´ì§€ì— ì£¼ì…í•©ë‹ˆë‹¤."""
        try:
            cookie_file = Path("cookies.json")
            if not cookie_file.exists():
                logger.warning("cookies.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¿ í‚¤ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
                return

            with open(cookie_file, 'r', encoding='utf-8') as f:
                cookies_data = json.load(f)

            # EditThisCookie í˜•ì‹ì˜ ì¤‘ì²© ë°°ì—´ ì²˜ë¦¬
            if isinstance(cookies_data, list) and len(cookies_data) > 0:
                if isinstance(cookies_data[0], list):
                    cookies_data = cookies_data[0]

            # Playwright í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€
            playwright_cookies = []
            for cookie in cookies_data:
                playwright_cookie = {
                    'name': cookie['name'],
                    'value': cookie['value'],
                    'domain': cookie['domain'],
                    'path': cookie.get('path', '/'),
                }

                # ì„ íƒì  í•„ë“œ ì¶”ê°€
                if 'expirationDate' in cookie:
                    playwright_cookie['expires'] = cookie['expirationDate']
                if 'httpOnly' in cookie:
                    playwright_cookie['httpOnly'] = cookie['httpOnly']
                if 'secure' in cookie:
                    playwright_cookie['secure'] = cookie['secure']
                if 'sameSite' in cookie and cookie['sameSite'] != 'unspecified':
                    # sameSite ê°’ ë³€í™˜
                    same_site_map = {
                        'no_restriction': 'None',
                        'lax': 'Lax',
                        'strict': 'Strict'
                    }
                    playwright_cookie['sameSite'] = same_site_map.get(cookie['sameSite'], 'Lax')

                playwright_cookies.append(playwright_cookie)

            # ì¿ í‚¤ ì¶”ê°€
            await self.page.context.add_cookies(playwright_cookies)
            logger.info(f"âœ… {len(playwright_cookies)}ê°œì˜ ì¿ í‚¤ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

            # ì¤‘ìš”í•œ ì¿ í‚¤ í™•ì¸
            important_cookies = ['__cf_bm', '_cfuvid', 'cf_clearance']
            loaded_important = [c['name'] for c in playwright_cookies if c['name'] in important_cookies]
            if loaded_important:
                logger.info(f"ğŸ”‘ Cloudflare ì¿ í‚¤ ë¡œë“œë¨: {', '.join(loaded_important)}")

        except Exception as e:
            logger.warning(f"ì¿ í‚¤ ë¡œë“œ ì‹¤íŒ¨: {e}. ì¿ í‚¤ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
